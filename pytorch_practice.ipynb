{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+aQnrlPj6l95r1qHALvy8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahabZada/learningPytorch/blob/main/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RszYoMFOIN92"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "dshape=(1,1)\n",
        "x=torch.rand(1,100)\n",
        "w=torch.rand(dshape, requires_grad=True)\n",
        "for i in range(1000):\n",
        "  y_pred=x*w\n",
        "\n",
        "print(x)\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUUAYXrQZh8i",
        "outputId": "95594340-1d14-4474-e6e5-25cdfe78ec92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999, 0.3971, 0.7544, 0.5695,\n",
            "         0.4388, 0.6387, 0.5247, 0.6826, 0.3051, 0.4635, 0.4550, 0.5725, 0.4980,\n",
            "         0.9371, 0.6556, 0.3138, 0.1980, 0.4162, 0.2843, 0.3398, 0.5239, 0.7981,\n",
            "         0.7718, 0.0112, 0.8100, 0.6397, 0.9743, 0.8300, 0.0444, 0.0246, 0.2588,\n",
            "         0.9391, 0.4167, 0.7140, 0.2676, 0.9906, 0.2885, 0.8750, 0.5059, 0.2366,\n",
            "         0.7570, 0.2346, 0.6471, 0.3556, 0.4452, 0.0193, 0.2616, 0.7713, 0.3785,\n",
            "         0.9980, 0.9008, 0.4766, 0.1663, 0.8045, 0.6552, 0.1768, 0.8248, 0.8036,\n",
            "         0.9434, 0.2197, 0.4177, 0.4903, 0.5730, 0.1205, 0.1452, 0.7720, 0.3828,\n",
            "         0.7442, 0.5285, 0.6642, 0.6099, 0.6818, 0.7479, 0.0369, 0.7517, 0.1484,\n",
            "         0.1227, 0.5304, 0.4148, 0.7937, 0.2104, 0.0555, 0.8639, 0.4259, 0.7812,\n",
            "         0.6607, 0.1251, 0.6004, 0.6201, 0.1652, 0.2628, 0.6705, 0.5896, 0.2873,\n",
            "         0.3486]])\n",
            "tensor([[0.9579]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a * x\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of b with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = (grad_y_pred * x).sum()\n",
        "    \n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    \n",
        "    a -= learning_rate * grad_a\n",
        "    \n",
        "\n",
        "print(f'Result: y =  {a.item()} x ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pONXOdT0kod8",
        "outputId": "3b159c03-e93f-499a-e49f-827c404b5d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 395.1315002441406\n",
            "199 392.95391845703125\n",
            "299 392.8004150390625\n",
            "399 392.78955078125\n",
            "499 392.7887878417969\n",
            "599 392.78875732421875\n",
            "699 392.7887268066406\n",
            "799 392.7886962890625\n",
            "899 392.7887268066406\n",
            "999 392.7887268066406\n",
            "1099 392.7887268066406\n",
            "1199 392.7887268066406\n",
            "1299 392.7887268066406\n",
            "1399 392.7887268066406\n",
            "1499 392.7887268066406\n",
            "1599 392.7887268066406\n",
            "1699 392.7887268066406\n",
            "1799 392.7887268066406\n",
            "1899 392.7887268066406\n",
            "1999 392.7887268066406\n",
            "Result: y =  0.3035065233707428 x \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#linear function in pytorch from scratch\n",
        "\n",
        "import torch\n",
        "\n",
        "# f = w * x \n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype = torch.float32)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "# J = MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N * 2x(w*x - y)\n",
        "def gradient(x, y, y_pred):\n",
        "    return torch.dot(2*x, y_pred - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "    \n",
        "    # calculate gradients\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "\n",
        "    # update weights\n",
        "    w -= learning_rate * dw\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "     \n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ScB_taSODZ",
        "outputId": "a3b23c5f-4a30-4305-c0e5-c60f8ff5bfbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 7: w = 1.997, loss = 0.00050332\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 11: w = 2.000, loss = 0.00000033\n",
            "epoch 13: w = 2.000, loss = 0.00000001\n",
            "epoch 15: w = 2.000, loss = 0.00000000\n",
            "epoch 17: w = 2.000, loss = 0.00000000\n",
            "epoch 19: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#linear function in pytorch using autograd\n",
        "import torch\n",
        "\n",
        "# Here we replace the manually computed gradient with autograd\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x \n",
        "\n",
        "# here : f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    #w.data = w.data - learning_rate * w.grad\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "    \n",
        "    # zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMRppEveeYvA",
        "outputId": "b32ccfd8-a4e2-4943-9785-a4aca43fc4ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "X=torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y=torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "#model output\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "#loss\n",
        "def loss(y, y_pred):\n",
        "  return mse_loss(y, y_pred)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iters=1000\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  #gradients calculation\n",
        "  l.backward()\n",
        "\n",
        "  #update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  #zero the gradients after updating\n",
        "  w.grad.zero_()\n",
        "\n",
        "\n",
        "  if epoch % 100 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "    \n",
        "print(f'Prediction after training = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HatN8ElLfpUy",
        "outputId": "6dd44c94-8fbe-40f9-a81f-9fcf1fcf8131"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 101: w = 2.000, loss = 0.00000000\n",
            "epoch 201: w = 2.000, loss = 0.00000000\n",
            "epoch 301: w = 2.000, loss = 0.00000000\n",
            "epoch 401: w = 2.000, loss = 0.00000000\n",
            "epoch 501: w = 2.000, loss = 0.00000000\n",
            "epoch 601: w = 2.000, loss = 0.00000000\n",
            "epoch 701: w = 2.000, loss = 0.00000000\n",
            "epoch 801: w = 2.000, loss = 0.00000000\n",
            "epoch 901: w = 2.000, loss = 0.00000000\n",
            "Prediction after training = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using pytorch builtin loss function and optimizer\n",
        "###\n",
        "#1) design model (input, output size, forward pass)\n",
        "#2) construct loss and optimizer\n",
        "#3) training loop\n",
        "#      forward pass -compute prediction\n",
        "#      backward pass -gradients\n",
        "#      update weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "X=torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y=torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "#model output\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iters=1000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer= torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  #gradients calculation\n",
        "  l.backward()\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero the gradients after updating\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if epoch % 100 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "    \n",
        "print(f'Prediction after training = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2bZ9FanZxR",
        "outputId": "58bec94e-6e22-454b-85da-a4ccb89a62c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 101: w = 2.000, loss = 0.00000000\n",
            "epoch 201: w = 2.000, loss = 0.00000000\n",
            "epoch 301: w = 2.000, loss = 0.00000000\n",
            "epoch 401: w = 2.000, loss = 0.00000000\n",
            "epoch 501: w = 2.000, loss = 0.00000000\n",
            "epoch 601: w = 2.000, loss = 0.00000000\n",
            "epoch 701: w = 2.000, loss = 0.00000000\n",
            "epoch 801: w = 2.000, loss = 0.00000000\n",
            "epoch 901: w = 2.000, loss = 0.00000000\n",
            "Prediction after training = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) Prepare data\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
        "\n",
        "# cast to float Tensor\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# 1) Model\n",
        "# Linear model f = wx + b\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) Loss and optimizer\n",
        "learning_rate = 0.01\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# 3) Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "    \n",
        "    # Backward pass and update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# Plot\n",
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "YkzIDfeVu2rc",
        "outputId": "0e460b16-f3a0-4c3b-f5e7-562dec832b49"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 3979.0012\n",
            "epoch: 20, loss = 2806.3831\n",
            "epoch: 30, loss = 2006.8636\n",
            "epoch: 40, loss = 1461.6179\n",
            "epoch: 50, loss = 1089.7017\n",
            "epoch: 60, loss = 835.9637\n",
            "epoch: 70, loss = 662.8187\n",
            "epoch: 80, loss = 544.6459\n",
            "epoch: 90, loss = 463.9770\n",
            "epoch: 100, loss = 408.8992\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#0) Prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "#scale\n",
        "sc= StandardScaler()\n",
        "X_train=sc.fit_transform(X_train)\n",
        "X_test=sc.fit_transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0],1)\n",
        "y_test = y_test.view(y_test.shape[0],1)\n",
        "\n",
        "#1) Model\n",
        "#Linear model f=wx+b , sigmoid at end\n",
        "class LogesticRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogesticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogesticRegression(n_features)\n",
        "\n",
        "\n",
        "#2) loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss() #binary crossentropy loss\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#3)training loop\n",
        "num_epochs = 10000\n",
        "for epoch in range(num_epochs):\n",
        "  #forward pass\n",
        "  y_predicted = model(X_train)\n",
        "  loss = criterion(y_predicted, y_train)\n",
        "\n",
        "  #backward pass \n",
        "  loss.backward()\n",
        "\n",
        "  #updates\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if(epoch+1)%1000==0:\n",
        "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "  acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'accuracy = {acc:.4f}')   \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cer5Fnb_yXB4",
        "outputId": "a21b61ed-0c91-46de-85cc-d8d709f9dcd8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1000, loss = 0.0875\n",
            "epoch: 2000, loss = 0.0681\n",
            "epoch: 3000, loss = 0.0593\n",
            "epoch: 4000, loss = 0.0539\n",
            "epoch: 5000, loss = 0.0502\n",
            "epoch: 6000, loss = 0.0474\n",
            "epoch: 7000, loss = 0.0452\n",
            "epoch: 8000, loss = 0.0433\n",
            "epoch: 9000, loss = 0.0418\n",
            "epoch: 10000, loss = 0.0405\n",
            "accuracy = 0.9649\n"
          ]
        }
      ]
    }
  ]
}