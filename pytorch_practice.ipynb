{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEF13Oe2xgzUEVZciBAH1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahabZada/learningPytorch/blob/main/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RszYoMFOIN92"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "dshape=(1,1)\n",
        "x=torch.rand(1,100)\n",
        "w=torch.rand(dshape, requires_grad=True)\n",
        "for i in range(1000):\n",
        "  y_pred=x*w\n",
        "\n",
        "print(x)\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUUAYXrQZh8i",
        "outputId": "95594340-1d14-4474-e6e5-25cdfe78ec92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999, 0.3971, 0.7544, 0.5695,\n",
            "         0.4388, 0.6387, 0.5247, 0.6826, 0.3051, 0.4635, 0.4550, 0.5725, 0.4980,\n",
            "         0.9371, 0.6556, 0.3138, 0.1980, 0.4162, 0.2843, 0.3398, 0.5239, 0.7981,\n",
            "         0.7718, 0.0112, 0.8100, 0.6397, 0.9743, 0.8300, 0.0444, 0.0246, 0.2588,\n",
            "         0.9391, 0.4167, 0.7140, 0.2676, 0.9906, 0.2885, 0.8750, 0.5059, 0.2366,\n",
            "         0.7570, 0.2346, 0.6471, 0.3556, 0.4452, 0.0193, 0.2616, 0.7713, 0.3785,\n",
            "         0.9980, 0.9008, 0.4766, 0.1663, 0.8045, 0.6552, 0.1768, 0.8248, 0.8036,\n",
            "         0.9434, 0.2197, 0.4177, 0.4903, 0.5730, 0.1205, 0.1452, 0.7720, 0.3828,\n",
            "         0.7442, 0.5285, 0.6642, 0.6099, 0.6818, 0.7479, 0.0369, 0.7517, 0.1484,\n",
            "         0.1227, 0.5304, 0.4148, 0.7937, 0.2104, 0.0555, 0.8639, 0.4259, 0.7812,\n",
            "         0.6607, 0.1251, 0.6004, 0.6201, 0.1652, 0.2628, 0.6705, 0.5896, 0.2873,\n",
            "         0.3486]])\n",
            "tensor([[0.9579]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a * x\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of b with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = (grad_y_pred * x).sum()\n",
        "    \n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    \n",
        "    a -= learning_rate * grad_a\n",
        "    \n",
        "\n",
        "print(f'Result: y =  {a.item()} x ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pONXOdT0kod8",
        "outputId": "3b159c03-e93f-499a-e49f-827c404b5d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 395.1315002441406\n",
            "199 392.95391845703125\n",
            "299 392.8004150390625\n",
            "399 392.78955078125\n",
            "499 392.7887878417969\n",
            "599 392.78875732421875\n",
            "699 392.7887268066406\n",
            "799 392.7886962890625\n",
            "899 392.7887268066406\n",
            "999 392.7887268066406\n",
            "1099 392.7887268066406\n",
            "1199 392.7887268066406\n",
            "1299 392.7887268066406\n",
            "1399 392.7887268066406\n",
            "1499 392.7887268066406\n",
            "1599 392.7887268066406\n",
            "1699 392.7887268066406\n",
            "1799 392.7887268066406\n",
            "1899 392.7887268066406\n",
            "1999 392.7887268066406\n",
            "Result: y =  0.3035065233707428 x \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#linear function in pytorch from scratch\n",
        "\n",
        "import torch\n",
        "\n",
        "# f = w * x \n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype = torch.float32)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "# J = MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N * 2x(w*x - y)\n",
        "def gradient(x, y, y_pred):\n",
        "    return torch.dot(2*x, y_pred - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "    \n",
        "    # calculate gradients\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "\n",
        "    # update weights\n",
        "    w -= learning_rate * dw\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "     \n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ScB_taSODZ",
        "outputId": "a3b23c5f-4a30-4305-c0e5-c60f8ff5bfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 7: w = 1.997, loss = 0.00050332\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 11: w = 2.000, loss = 0.00000033\n",
            "epoch 13: w = 2.000, loss = 0.00000001\n",
            "epoch 15: w = 2.000, loss = 0.00000000\n",
            "epoch 17: w = 2.000, loss = 0.00000000\n",
            "epoch 19: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#linear function in pytorch using autograd\n",
        "import torch\n",
        "\n",
        "# Here we replace the manually computed gradient with autograd\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x \n",
        "\n",
        "# here : f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    #w.data = w.data - learning_rate * w.grad\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "    \n",
        "    # zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMRppEveeYvA",
        "outputId": "b32ccfd8-a4e2-4943-9785-a4aca43fc4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "X=torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y=torch.tensor([1, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "#model output\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "#loss\n",
        "def loss(y, y_pred):\n",
        "  return mse_loss(y, y_pred)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iters=1000\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  #gradients calculation\n",
        "  l.backward()\n",
        "\n",
        "  #update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  #zero the gradients after updating\n",
        "  w.grad.zero_()\n",
        "\n",
        "\n",
        "  if epoch % 10 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "    \n",
        "print(f'Prediction after training = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HatN8ElLfpUy",
        "outputId": "d26d1dc5-ef81-4871-86c1-405576d53e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.295, loss = 29.25000000\n",
            "epoch 11: w = 1.638, loss = 1.36601555\n",
            "epoch 21: w = 1.902, loss = 0.28524587\n",
            "epoch 31: w = 1.954, loss = 0.24335578\n",
            "epoch 41: w = 1.964, loss = 0.24173214\n",
            "epoch 51: w = 1.966, loss = 0.24166919\n",
            "epoch 61: w = 1.967, loss = 0.24166678\n",
            "epoch 71: w = 1.967, loss = 0.24166670\n",
            "epoch 81: w = 1.967, loss = 0.24166667\n",
            "epoch 91: w = 1.967, loss = 0.24166666\n",
            "epoch 101: w = 1.967, loss = 0.24166666\n",
            "epoch 111: w = 1.967, loss = 0.24166666\n",
            "epoch 121: w = 1.967, loss = 0.24166666\n",
            "epoch 131: w = 1.967, loss = 0.24166666\n",
            "epoch 141: w = 1.967, loss = 0.24166666\n",
            "epoch 151: w = 1.967, loss = 0.24166666\n",
            "epoch 161: w = 1.967, loss = 0.24166666\n",
            "epoch 171: w = 1.967, loss = 0.24166666\n",
            "epoch 181: w = 1.967, loss = 0.24166666\n",
            "epoch 191: w = 1.967, loss = 0.24166666\n",
            "epoch 201: w = 1.967, loss = 0.24166666\n",
            "epoch 211: w = 1.967, loss = 0.24166666\n",
            "epoch 221: w = 1.967, loss = 0.24166666\n",
            "epoch 231: w = 1.967, loss = 0.24166666\n",
            "epoch 241: w = 1.967, loss = 0.24166666\n",
            "epoch 251: w = 1.967, loss = 0.24166666\n",
            "epoch 261: w = 1.967, loss = 0.24166666\n",
            "epoch 271: w = 1.967, loss = 0.24166666\n",
            "epoch 281: w = 1.967, loss = 0.24166666\n",
            "epoch 291: w = 1.967, loss = 0.24166666\n",
            "epoch 301: w = 1.967, loss = 0.24166666\n",
            "epoch 311: w = 1.967, loss = 0.24166666\n",
            "epoch 321: w = 1.967, loss = 0.24166666\n",
            "epoch 331: w = 1.967, loss = 0.24166666\n",
            "epoch 341: w = 1.967, loss = 0.24166666\n",
            "epoch 351: w = 1.967, loss = 0.24166666\n",
            "epoch 361: w = 1.967, loss = 0.24166666\n",
            "epoch 371: w = 1.967, loss = 0.24166666\n",
            "epoch 381: w = 1.967, loss = 0.24166666\n",
            "epoch 391: w = 1.967, loss = 0.24166666\n",
            "epoch 401: w = 1.967, loss = 0.24166666\n",
            "epoch 411: w = 1.967, loss = 0.24166666\n",
            "epoch 421: w = 1.967, loss = 0.24166666\n",
            "epoch 431: w = 1.967, loss = 0.24166666\n",
            "epoch 441: w = 1.967, loss = 0.24166666\n",
            "epoch 451: w = 1.967, loss = 0.24166666\n",
            "epoch 461: w = 1.967, loss = 0.24166666\n",
            "epoch 471: w = 1.967, loss = 0.24166666\n",
            "epoch 481: w = 1.967, loss = 0.24166666\n",
            "epoch 491: w = 1.967, loss = 0.24166666\n",
            "epoch 501: w = 1.967, loss = 0.24166666\n",
            "epoch 511: w = 1.967, loss = 0.24166666\n",
            "epoch 521: w = 1.967, loss = 0.24166666\n",
            "epoch 531: w = 1.967, loss = 0.24166666\n",
            "epoch 541: w = 1.967, loss = 0.24166666\n",
            "epoch 551: w = 1.967, loss = 0.24166666\n",
            "epoch 561: w = 1.967, loss = 0.24166666\n",
            "epoch 571: w = 1.967, loss = 0.24166666\n",
            "epoch 581: w = 1.967, loss = 0.24166666\n",
            "epoch 591: w = 1.967, loss = 0.24166666\n",
            "epoch 601: w = 1.967, loss = 0.24166666\n",
            "epoch 611: w = 1.967, loss = 0.24166666\n",
            "epoch 621: w = 1.967, loss = 0.24166666\n",
            "epoch 631: w = 1.967, loss = 0.24166666\n",
            "epoch 641: w = 1.967, loss = 0.24166666\n",
            "epoch 651: w = 1.967, loss = 0.24166666\n",
            "epoch 661: w = 1.967, loss = 0.24166666\n",
            "epoch 671: w = 1.967, loss = 0.24166666\n",
            "epoch 681: w = 1.967, loss = 0.24166666\n",
            "epoch 691: w = 1.967, loss = 0.24166666\n",
            "epoch 701: w = 1.967, loss = 0.24166666\n",
            "epoch 711: w = 1.967, loss = 0.24166666\n",
            "epoch 721: w = 1.967, loss = 0.24166666\n",
            "epoch 731: w = 1.967, loss = 0.24166666\n",
            "epoch 741: w = 1.967, loss = 0.24166666\n",
            "epoch 751: w = 1.967, loss = 0.24166666\n",
            "epoch 761: w = 1.967, loss = 0.24166666\n",
            "epoch 771: w = 1.967, loss = 0.24166666\n",
            "epoch 781: w = 1.967, loss = 0.24166666\n",
            "epoch 791: w = 1.967, loss = 0.24166666\n",
            "epoch 801: w = 1.967, loss = 0.24166666\n",
            "epoch 811: w = 1.967, loss = 0.24166666\n",
            "epoch 821: w = 1.967, loss = 0.24166666\n",
            "epoch 831: w = 1.967, loss = 0.24166666\n",
            "epoch 841: w = 1.967, loss = 0.24166666\n",
            "epoch 851: w = 1.967, loss = 0.24166666\n",
            "epoch 861: w = 1.967, loss = 0.24166666\n",
            "epoch 871: w = 1.967, loss = 0.24166666\n",
            "epoch 881: w = 1.967, loss = 0.24166666\n",
            "epoch 891: w = 1.967, loss = 0.24166666\n",
            "epoch 901: w = 1.967, loss = 0.24166666\n",
            "epoch 911: w = 1.967, loss = 0.24166666\n",
            "epoch 921: w = 1.967, loss = 0.24166666\n",
            "epoch 931: w = 1.967, loss = 0.24166666\n",
            "epoch 941: w = 1.967, loss = 0.24166666\n",
            "epoch 951: w = 1.967, loss = 0.24166666\n",
            "epoch 961: w = 1.967, loss = 0.24166666\n",
            "epoch 971: w = 1.967, loss = 0.24166666\n",
            "epoch 981: w = 1.967, loss = 0.24166666\n",
            "epoch 991: w = 1.967, loss = 0.24166666\n",
            "Prediction after training = 9.833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_oV-6VkplaKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}