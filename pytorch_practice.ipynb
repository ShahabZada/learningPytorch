{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJ7HLjfm0/pTT4fnlnPJfd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahabZada/learningPytorch/blob/main/pytorch_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RszYoMFOIN92"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "dshape=(1,1)\n",
        "x=torch.rand(1,100)\n",
        "w=torch.rand(dshape, requires_grad=True)\n",
        "for i in range(1000):\n",
        "  y_pred=x*w\n",
        "\n",
        "print(x)\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUUAYXrQZh8i",
        "outputId": "95594340-1d14-4474-e6e5-25cdfe78ec92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999, 0.3971, 0.7544, 0.5695,\n",
            "         0.4388, 0.6387, 0.5247, 0.6826, 0.3051, 0.4635, 0.4550, 0.5725, 0.4980,\n",
            "         0.9371, 0.6556, 0.3138, 0.1980, 0.4162, 0.2843, 0.3398, 0.5239, 0.7981,\n",
            "         0.7718, 0.0112, 0.8100, 0.6397, 0.9743, 0.8300, 0.0444, 0.0246, 0.2588,\n",
            "         0.9391, 0.4167, 0.7140, 0.2676, 0.9906, 0.2885, 0.8750, 0.5059, 0.2366,\n",
            "         0.7570, 0.2346, 0.6471, 0.3556, 0.4452, 0.0193, 0.2616, 0.7713, 0.3785,\n",
            "         0.9980, 0.9008, 0.4766, 0.1663, 0.8045, 0.6552, 0.1768, 0.8248, 0.8036,\n",
            "         0.9434, 0.2197, 0.4177, 0.4903, 0.5730, 0.1205, 0.1452, 0.7720, 0.3828,\n",
            "         0.7442, 0.5285, 0.6642, 0.6099, 0.6818, 0.7479, 0.0369, 0.7517, 0.1484,\n",
            "         0.1227, 0.5304, 0.4148, 0.7937, 0.2104, 0.0555, 0.8639, 0.4259, 0.7812,\n",
            "         0.6607, 0.1251, 0.6004, 0.6201, 0.1652, 0.2628, 0.6705, 0.5896, 0.2873,\n",
            "         0.3486]])\n",
            "tensor([[0.9579]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
        "y = torch.sin(x)\n",
        "\n",
        "# Randomly initialize weights\n",
        "\n",
        "a = torch.randn((), device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = a * x\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of b with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = (grad_y_pred * x).sum()\n",
        "    \n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    \n",
        "    a -= learning_rate * grad_a\n",
        "    \n",
        "\n",
        "print(f'Result: y =  {a.item()} x ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pONXOdT0kod8",
        "outputId": "3b159c03-e93f-499a-e49f-827c404b5d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99 395.1315002441406\n",
            "199 392.95391845703125\n",
            "299 392.8004150390625\n",
            "399 392.78955078125\n",
            "499 392.7887878417969\n",
            "599 392.78875732421875\n",
            "699 392.7887268066406\n",
            "799 392.7886962890625\n",
            "899 392.7887268066406\n",
            "999 392.7887268066406\n",
            "1099 392.7887268066406\n",
            "1199 392.7887268066406\n",
            "1299 392.7887268066406\n",
            "1399 392.7887268066406\n",
            "1499 392.7887268066406\n",
            "1599 392.7887268066406\n",
            "1699 392.7887268066406\n",
            "1799 392.7887268066406\n",
            "1899 392.7887268066406\n",
            "1999 392.7887268066406\n",
            "Result: y =  0.3035065233707428 x \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#linear function in pytorch from scratch\n",
        "\n",
        "import torch\n",
        "\n",
        "# f = w * x \n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype = torch.float32)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "# J = MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N * 2x(w*x - y)\n",
        "def gradient(x, y, y_pred):\n",
        "    return torch.dot(2*x, y_pred - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "    \n",
        "    # calculate gradients\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "\n",
        "    # update weights\n",
        "    w -= learning_rate * dw\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "     \n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ScB_taSODZ",
        "outputId": "a3b23c5f-4a30-4305-c0e5-c60f8ff5bfbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 7: w = 1.997, loss = 0.00050332\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 11: w = 2.000, loss = 0.00000033\n",
            "epoch 13: w = 2.000, loss = 0.00000001\n",
            "epoch 15: w = 2.000, loss = 0.00000000\n",
            "epoch 17: w = 2.000, loss = 0.00000000\n",
            "epoch 19: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#linear function in pytorch using autograd\n",
        "import torch\n",
        "\n",
        "# Here we replace the manually computed gradient with autograd\n",
        "\n",
        "# Linear regression\n",
        "# f = w * x \n",
        "\n",
        "# here : f = 2 * x\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model output\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    # predict = forward pass\n",
        "    y_pred = forward(X)\n",
        "\n",
        "    # loss\n",
        "    l = loss(Y, y_pred)\n",
        "\n",
        "    # calculate gradients = backward pass\n",
        "    l.backward()\n",
        "\n",
        "    # update weights\n",
        "    #w.data = w.data - learning_rate * w.grad\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "    \n",
        "    # zero the gradients after updating\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMRppEveeYvA",
        "outputId": "b32ccfd8-a4e2-4943-9785-a4aca43fc4ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 61: w = 2.000, loss = 0.00000010\n",
            "epoch 71: w = 2.000, loss = 0.00000000\n",
            "epoch 81: w = 2.000, loss = 0.00000000\n",
            "epoch 91: w = 2.000, loss = 0.00000000\n",
            "Prediction after training: f(5) = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "X=torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y=torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "#model output\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "#loss\n",
        "def loss(y, y_pred):\n",
        "  return mse_loss(y, y_pred)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iters=1000\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  #gradients calculation\n",
        "  l.backward()\n",
        "\n",
        "  #update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  #zero the gradients after updating\n",
        "  w.grad.zero_()\n",
        "\n",
        "\n",
        "  if epoch % 100 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "    \n",
        "print(f'Prediction after training = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HatN8ElLfpUy",
        "outputId": "6dd44c94-8fbe-40f9-a81f-9fcf1fcf8131"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 101: w = 2.000, loss = 0.00000000\n",
            "epoch 201: w = 2.000, loss = 0.00000000\n",
            "epoch 301: w = 2.000, loss = 0.00000000\n",
            "epoch 401: w = 2.000, loss = 0.00000000\n",
            "epoch 501: w = 2.000, loss = 0.00000000\n",
            "epoch 601: w = 2.000, loss = 0.00000000\n",
            "epoch 701: w = 2.000, loss = 0.00000000\n",
            "epoch 801: w = 2.000, loss = 0.00000000\n",
            "epoch 901: w = 2.000, loss = 0.00000000\n",
            "Prediction after training = 10.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using pytorch builtin loss function and optimizer\n",
        "###\n",
        "#1) design model (input, output size, forward pass)\n",
        "#2) construct loss and optimizer\n",
        "#3) training loop\n",
        "#      forward pass -compute prediction\n",
        "#      backward pass -gradients\n",
        "#      update weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "X=torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y=torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w=torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "#model output\n",
        "def forward(x):\n",
        "  return w*x\n",
        "\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iters=1000\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer= torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  #gradients calculation\n",
        "  l.backward()\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero the gradients after updating\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "\n",
        "  if epoch % 100 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "    \n",
        "print(f'Prediction after training = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2bZ9FanZxR",
        "outputId": "58bec94e-6e22-454b-85da-a4ccb89a62c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 101: w = 2.000, loss = 0.00000000\n",
            "epoch 201: w = 2.000, loss = 0.00000000\n",
            "epoch 301: w = 2.000, loss = 0.00000000\n",
            "epoch 401: w = 2.000, loss = 0.00000000\n",
            "epoch 501: w = 2.000, loss = 0.00000000\n",
            "epoch 601: w = 2.000, loss = 0.00000000\n",
            "epoch 701: w = 2.000, loss = 0.00000000\n",
            "epoch 801: w = 2.000, loss = 0.00000000\n",
            "epoch 901: w = 2.000, loss = 0.00000000\n",
            "Prediction after training = 10.000\n"
          ]
        }
      ]
    }
  ]
}